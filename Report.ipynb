{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: https://user-images.githubusercontent.com/10624937/42135619-d90f2f28-7d12-11e8-8823-82b970a54d7e.gif \"Trained Agent\"\n",
    "\n",
    "# Project 1: Continuous Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[image1]: https://user-images.githubusercontent.com/10624937/43851024-320ba930-9aff-11e8-8493-ee547c6af349.gif \"Trained Agent\"\n",
    "\n",
    "### Introduction\n",
    "\n",
    "For this project, you will work with the [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) environment.\n",
    "\n",
    "![Trained Agent][image1]\n",
    "  \n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.\n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those average scores is at least +30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDPG Agent\n",
    "DDPG agent, [ddpg_agent.py](ddpg_agent.py), implements the DDPG algorithm from [DDPG paper](https://arxiv.org/pdf/1509.02971). \n",
    "\n",
    "1. It is a policy gradient algorithm that employs actor-critic model.\n",
    "\n",
    "    Actor network parameters\n",
    "    ```\n",
    "    ----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "    ================================================================\n",
    "           BatchNorm1d-1                   [-1, 33]              66\n",
    "                Linear-2                  [-1, 256]           8,704\n",
    "                Linear-3                  [-1, 128]          32,896\n",
    "                Linear-4                    [-1, 4]             516\n",
    "    ================================================================\n",
    "    Total params: 42,182\n",
    "    Trainable params: 42,182\n",
    "    Non-trainable params: 0\n",
    "    ----------------------------------------------------------------\n",
    "    Input size (MB): 0.00\n",
    "    Forward/backward pass size (MB): 0.00\n",
    "    Params size (MB): 0.16\n",
    "    Estimated Total Size (MB): 0.16\n",
    "    ----------------------------------------------------------------\n",
    "    ```\n",
    "    \n",
    "    Critic network parameters\n",
    "    ```\n",
    "    ----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "    ================================================================\n",
    "           BatchNorm1d-1                   [-1, 33]              66\n",
    "                Linear-2                  [-1, 256]           8,704\n",
    "                Linear-3                  [-1, 128]          33,408\n",
    "                Linear-4                    [-1, 1]             129\n",
    "    ================================================================\n",
    "    Total params: 42,307\n",
    "    Trainable params: 42,307\n",
    "    Non-trainable params: 0\n",
    "    ----------------------------------------------------------------\n",
    "    Input size (MB): 0.00\n",
    "    Forward/backward pass size (MB): 0.00\n",
    "    Params size (MB): 0.16\n",
    "    Estimated Total Size (MB): 0.17\n",
    "    ----------------------------------------------------------------\n",
    "    ```\n",
    "2. DDPG is a off-policy learning as the target determinstic policy is different from the noisy learning behavior policy.\n",
    "3. Soft update is employed for both actor and critic network to achieve a stable learning process. The parameter of $\\tau$ is used to control how much we mix in from the behavior network into the target network each time.\n",
    "\n",
    "    $$\n",
    "    \\theta_{target} = \\tau*\\theta_{local} + (1-\\tau)*\\theta_{target}\n",
    "    $$\n",
    "    \n",
    "4. The critic network is learned with temporal difference(TD) approach.\n",
    "    $$\n",
    "    y_t = r_t + discount * Q'(s_{t+1},a,\\theta_t')\n",
    "    $$    $$\n",
    "    L^{critic} = \\frac{1}{N}\\sum(y_t - Q(s_t,a,\\theta_t))^2\n",
    "    $$\n",
    "\n",
    "5. The actor network is learned by maximizing objective function $J(\\theta)$. It means that we encourage the actions that produce bigger Q value, and on the other hand discourage the actions with smaller Q value:\n",
    "![Objective Function](./assets/objective_function.jpg)\n",
    "\n",
    "6. DDPG algorithm.\n",
    "![Algorithm](./assets/algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-Parameters\n",
    "* Replay buffer size, BUFFER_SIZE, of **100,000**.\n",
    "* Minibatch size, BATCH_SIZE, of **128**.\n",
    "* Discount factor, GAMMA, of **0.99**.\n",
    "* Soft update of target parameters, TAU, of **0.001**.\n",
    "* Actor learning rate of **0.0001** and critic learning rate of **0.001**.\n",
    "* Unmodified Ornstein-Uhlenbeck noise: mu of **0.0**, theta of **0.15** and sigma of **0.2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "The following is training result, which is resolved in 17 episodes with average score of 30.26 over the last 100 episodes.\n",
    "\n",
    "![Result](./assets/result.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements\n",
    "I am interested in using PPO to resolve the problem. Unfortunately, with lots of time put in trying to train a PPO actor-critic for PongDeterministicV4, I did not finally get it working. It can learn but having memory leak issues. The issue was posted [here](https://discuss.pytorch.org/t/constant-memory-leak/28753) and the repo is [here](https://github.com/weicheng113/PongPPO). Hopefully, I can get it working in future and be able to experiment more with PPO approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
